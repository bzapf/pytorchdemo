{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining neural networks with partial differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "+ Early work, basically PINNs:\n",
    "Lagaris, I. E., Likas, A., & Fotiadis, D. I. (1998). Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks\n",
    "\n",
    "+ Popularized PINN methodology:\n",
    "Raissi, M., Perdikaris, P., & **Karniadakis, G. E.** (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics\n",
    "\n",
    "+ **Cool application of PINNs in fluid mechanics:**\n",
    "Raissi, M., Yazdani, A., & Karniadakis, G. E. (2020). Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science\n",
    "\n",
    "+ Automatic differentiation (in ML):\n",
    "Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research\n",
    "\n",
    "+ The M.Sc. thesis by Sebastian Mitusch (Simula) gives a nice intro to forward/backward automatic differentiation (and implementation for FEniCS in dolfin-adjoint):\n",
    "https://www.duo.uio.no/handle/10852/63505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN methodology\n",
    "\n",
    "Solve PDE $\\mathcal{L}u=0$ for solution $u$ being a neural network, $u(x) = u^{NN}(x, \\theta)$ by formulating the PDE problem as optimization problem in $\\theta$:\n",
    "\n",
    "$\\min_{\\theta} L(\\theta) = \\min_{\\theta} \\sum_{x_i \\in \\partial\\Omega} (u^{NN}(x_i)-g(x_i))^2 + \\sum_{x_i \\in \\Omega} (\\mathcal{L}u^{NN}(x_i))^2$\n",
    "\n",
    "where $u = g $ for $x \\in \\partial\\Omega$ is a Dirichlet boundary condition.\n",
    "\n",
    "## Hence we need:\n",
    "\n",
    "1. Neural network $u^{NN}(\\theta) \\quad \\qquad \\qquad \\rightarrow \\qquad$ PyTorch: Tensors, Activation Functions, Parameters,  ... \n",
    "2. Derivatives $\\frac{du^{NN}}{dx}, ... \\qquad \\qquad \\qquad \\rightarrow \\qquad$ Automatic differentiation\n",
    "3. Algorithm to find $\\theta = \\text{argmin} L(\\theta) \\qquad \\rightarrow \\qquad $ Gradient-based optimization: SGD, L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if PyTorch finds a GPU (has to be NVIDIA):\n",
    "\n",
    "Note that for small data sets and small networks, CPU will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU \", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data structure, almost like numpy.arrays, but with support fo GPU and automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100, device=device)\n",
    "\n",
    "# Can also create tensors from numpy arrays:\n",
    "# x = torch.from_numpy(np.linspace(0,1,100))\n",
    "# x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can often do things in numpy style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[x < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden_units, num_hidden_layers, inputs, outputs=1):\n",
    "        \n",
    "        super(Net, self).__init__()        \n",
    "        \n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Dimensions of input/output\n",
    "        self.inputs =  inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        self.input_layer = torch.nn.Linear(self.inputs, self.num_hidden_units)\n",
    "        \n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(\n",
    "            self.num_hidden_units, self.num_hidden_units)\n",
    "            for i in range(self.num_hidden_layers - 1)])\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(self.num_hidden_units, self.outputs)\n",
    "        \n",
    "        # Use hyperbolic tangent as activation:\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"[Compute NN output]\n",
    "\n",
    "        Args:\n",
    "            x ([torch.Tensor]): input tensor\n",
    "        Returns:\n",
    "            [torch.Tensor]: [The NN output]\n",
    "        \"\"\"\n",
    "        # Transform the shape of the Tensor to match what is expected by torch.nn.Linear\n",
    "        x = torch.unsqueeze(x, 1) \n",
    "    \n",
    "        out = self.input_layer(x)\n",
    "        \n",
    "        # The first hidden layer:\n",
    "        out = self.activation(out)\n",
    "\n",
    "        # The other hidden layers:\n",
    "        for i, linearLayer in enumerate(self.linear_layers):\n",
    "            out = linearLayer(out)\n",
    "            out = self.activation(out)\n",
    "\n",
    "        # No activation in the output layer:\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        out = torch.squeeze(out, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feedforward NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=2, inputs=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the trainable parameters (weights and biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the weights in one layer:\n",
    "u_nn.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of tunable parameter per Parameter object\n",
    "params = list(u_nn.parameters())\n",
    "print(list(map(lambda x: x.numel(), params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the output of the untrained NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn_x = u_nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.tolist(), u_nn_x.tolist())\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"Network output\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Automatic (Algorithmic) differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Derivative of\n",
    "\n",
    "$ f(x) = \\exp(-3x^2) = v_4(v_3(v_2(v_1(x))))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Forward pass: Given $x$, compute       |      \n",
    "| ----| :----------------------------------------- | \n",
    "| $\\downarrow$ | $v_1 = x^2$   | \n",
    "| $\\downarrow$| $v_2 = 3 v_1$ | \n",
    "| $\\downarrow$| $v_3 = - v_2$ \n",
    "| $\\downarrow$| $v_4 = \\exp v_3$ | \n",
    "|  $\\downarrow$| $f = v_4 $ |$ \\overline{v_4} = \\frac{df }{d v_4} = 1$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the chain rule:\n",
    "\n",
    "$\\frac{df}{dx} =\\frac{d f}{d v_4} \\frac{d v_4}{d v_3}\\frac{d v_3}{d v_2}\\frac{d v_2}{d v_1}\\frac{d v_1}{dx}$\n",
    "\n",
    "and the fact that all the derivatives $\\frac{d v_{i+1}}{d v_i}$ of the **elementary operations $v_{i+1}(v_i)$** can be considered known!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to have $\\frac{df}{dx}$;\n",
    "\n",
    "Let us define $ \\overline{v_i} = \\frac{df}{dv_i}$ and start from  $ \\overline{v_4}$ to compute the other $ \\overline{v_i}$, utilizing \n",
    "+ the stored values $v_i$\n",
    "+ Known (part of the code base) derivatives $\\frac{d v_{i+1}}{d v_i}$ of the elementary operations $v_{i+1}(v_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"foo\">\n",
    "    \n",
    "|  <div style=\"width:200px\">Forward pass</div>        |           <div style=\"width:350px\">Backward pass</div>               | |\n",
    "| :----------------------------------------- | :----------------------------------------------------------| -----|\n",
    "| $v_1 = x^2$   | $ \\overline{x} = \\frac{d f}{d x} = \\overline{v_1}  \\frac{d v_1}{d x}= -3 \\exp (-3 x^2) \\cdot 2 x $ |$\\uparrow $ |\n",
    "| $v_2 = 3 v_1$ | $ \\overline{v_1} = \\overline{v_2}  \\frac{d v_2}{d v_1}= -\\exp (v_3) \\cdot 3$ |  $\\uparrow $ |\n",
    "| $v_3 = - v_2$ |$ \\overline{v_2} = \\overline{v_3}  \\frac{d v_3}{d v_2}= \\exp (v_3) \\cdot (-1)$ |$\\uparrow $ |\n",
    "| $v_4 = \\exp v_3$ | $ \\overline{v_3}= \\frac{d f}{d v_3} = \\frac{d f}{d v_4} \\frac{d v_4}{d v_3} =  \\overline{v_4}  \\frac{d v_4}{d v_3}= 1 \\cdot \\exp \\color{red} v_{ \\color{red} 3}  $ |$\\uparrow $ |\n",
    "| $f = v_4 $ |$ \\overline{f} = \\overline{v_4} = \\frac{df }{d v_4} = 1$| $\\uparrow $ |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be used to compute derivatives with respect to parameters:\n",
    "\n",
    "\n",
    "$ v_2 = \\theta \\cdot v_1 \\rightarrow  f_{\\theta}(x) = \\exp(-\\theta x^2)$\n",
    "\n",
    "$\\rightarrow \\frac{df_{\\theta}}{d \\theta} = \\frac{d f}{d v_4} \\frac{d v_4}{d v_3} \\frac{d v_3}{d v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} v_1 \n",
    "= - \\exp (- \\theta x^2 ) \\cdot  x^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch autograd to compute derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes: \n",
    "\n",
    "+ `requires_grad=True` tells PyTorch to keep track of intermediate results (\"create a computational graph\")\n",
    "\n",
    "+ In-place operations `x *= x` are not supported by autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2 # torch.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx, = torch.autograd.grad(outputs=y,\n",
    "                             inputs=x,\n",
    "                             grad_outputs=torch.ones_like(y),\n",
    "                             create_graph=True)\n",
    "\n",
    "ddy_dxx, = torch.autograd.grad(outputs=dy_dx,\n",
    "                             inputs=x,\n",
    "                             grad_outputs=torch.ones_like(dy_dx),\n",
    "                             create_graph=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark on the last arguments:\n",
    "\n",
    "1. `grad_outputs=torch.ones_like(y)` is required for non-scalar outputs, technical reasons related to backward automatic differentiation\n",
    "2. We need to set `create_graph=True` if we want to compute higher order derivatives such that PyTorch adds its computation of `dy_dx` to the computational graph (The same applies if we want to include derivatives into the loss function, because then we need derivatives with respect to NN weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sometimes want to convert the torch.Tensor back to python types (or numpy arrays):\n",
    "# Note that a GPU tensor needs to be moved to CPU with the .cpu() call\n",
    "\n",
    "dy_dx_list = dy_dx.tolist()\n",
    "dy_dx_np = dy_dx.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.tolist(), y.tolist(), label=\"y(x)\") \n",
    "plt.plot(x.tolist(), dy_dx.tolist(), label=\"dy/dx\") \n",
    "plt.plot(x.tolist(), ddy_dxx.tolist(), label=\"ddy/dxx\") \n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based optimization in a nutshell:\n",
    "\n",
    "while criterion is True:\n",
    "   1. Compute loss $L(\\theta, \\text{inputs})$\n",
    "   2. Update the parameters as $\\theta \\hookleftarrow \\theta - \\eta \\nabla_{\\theta} L (\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's \"train\" a NN to to fit the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(0, 1, 100, device=device)\n",
    "\n",
    "targets = torch.sin(np.pi * inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=1, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=torch.nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iters):\n",
    "    #  Free all intermediate values:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass:\n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    # Compute the MSE loss:\n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    # Backward pass, compute gradient w.r.t. weights and biases\n",
    "    loss.backward()\n",
    "    \n",
    "    # Log the loss to make a figure\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses);\n",
    "plt.xlabel(\"Iter\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), label=\"Network\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), label=\"Target\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a second-order method like L-BFGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=1, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim = torch.optim.LBFGS(params,\n",
    "                                max_iter=max_iters,\n",
    "                                line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L-BFGS needs to be used slightly different than first order methods in PyTorch:\n",
    "\n",
    "The iteration happens inside `lbfgs_optim.step()`, so it is sufficient to call `lbfgs_optim.step()` only once, and specify the number of iterations with the argument `max_iter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS requires one to define a method (called \"closure\" in the docs) \n",
    "# that evaluates the model and returns the loss:\n",
    "\n",
    "def closure(loss_function=torch.nn.MSELoss(reduction=\"mean\")):\n",
    "    \n",
    "    lbfgs_optim.zero_grad()\n",
    "    \n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        \n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call L-BFGS and plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim.step(closure)\n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), marker=\"o\", markevery=5, label=\"Network $y^{NN}(x)$\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), marker=\"s\", markevery=10, label=\"Target $y(x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solve PDE with NN in PINN-fashion\n",
    "\n",
    "Consider $- \\Delta u(x) = f(x) = \\sin \\pi x \\text{, }  x \\in \\Omega = [0,1], u=g=0 \\text{ on } \\Omega $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boundary points and values u(x) on boundary:\n",
    "boundary_points = torch.tensor([0., 1.], device=device)\n",
    "boundary_values = torch.zeros_like(boundary_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define interior points:\n",
    "residual_points = torch.linspace(0, 1, 100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define force term:\n",
    "f = torch.sin(np.pi * residual_points)\n",
    "\n",
    "# True solution \n",
    "true_solution = torch.sin(np.pi * residual_points) / (np.pi**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_solution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Network and construct an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=2, inputs=1).to(device)\n",
    "\n",
    "params = list(u_nn.parameters())\n",
    "\n",
    "loss_function=torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "lbfgs_optim = torch.optim.LBFGS(params,\n",
    "                                max_iter=1000,\n",
    "                                line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the boundary loss and pde residual loss needed for the PINN \n",
    "\n",
    "Note that these must be callables that return the loss, cf. the docs for torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_loss(nn, boundary_points, boundary_values):\n",
    "    \n",
    "    # Evaluate the NN at the boundary:\n",
    "    predictions = nn(boundary_points)\n",
    "    \n",
    "    return loss_function(predictions, boundary_values)\n",
    "\n",
    "def pde_loss(nn, residual_points, f):\n",
    "    \n",
    "    # We want to compute derivatives with respect to the input:\n",
    "    residual_points.requires_grad = True\n",
    "    \n",
    "    # Evaluate NN:\n",
    "    u = nn(residual_points)\n",
    "    \n",
    "    print(u.shape)\n",
    "    print(residual_points.shape)\n",
    "    \n",
    "    # Compute gradients, note the create_graph=True (it defaults to False)\n",
    "    du_dx, = torch.autograd.grad(outputs=u,\n",
    "                             inputs=residual_points,\n",
    "                             grad_outputs=torch.ones_like(residual_points),\n",
    "                             create_graph=True)\n",
    "\n",
    "    ddu_dxx, = torch.autograd.grad(outputs=du_dx,\n",
    "                                 inputs=residual_points,\n",
    "                                 grad_outputs=torch.ones_like(residual_points),\n",
    "                                 create_graph=True)\n",
    "    \n",
    "    # The residual corresponding to -d^2 u/ dx^2 = f\n",
    "    residual = ddu_dxx + f\n",
    "    print(residual.shape)\n",
    "    # Evaluate \\sum (-d^2 u/ dx^2 - f - 0)^2 (could also do something like torch.mean(residual ** 2))\n",
    "    return loss_function(residual, torch.zeros_like(residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    \n",
    "    lbfgs_optim.zero_grad()\n",
    "    \n",
    "    # Compute losses:\n",
    "    boundary_loss_value = boundary_loss(u_nn, boundary_points, boundary_values)\n",
    "    \n",
    "    pde_loss_value = pde_loss(u_nn, residual_points, f)\n",
    "    \n",
    "    loss = boundary_loss_value + pde_loss_value\n",
    "    \n",
    "    # Compute gradients of the loss w.r.t weights:\n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "    \n",
    "    # Log the loss:\n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize and plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(0,1, 400)\n",
    "true_solution = torch.sin(np.pi * inputs) / (np.pi**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), marker=\"o\", markevery=5, label=\"PINN solution\")\n",
    "plt.plot(inputs.tolist(), true_solution.tolist(), marker=\"s\", markevery=10, label=\"True PDE solution\")\n",
    "plt.xlabel(\"x\")\n",
    "# plt.xlim(0, 0.1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inverse Problem\n",
    "\n",
    "Let us consider dynamics of an object in 1-D under gravitation:\n",
    "\n",
    "$F = m \\ddot x = -mg$.\n",
    "\n",
    "The trajectory is then $y(t) = -\\frac{1}{2} g t^2 + v_0 t$ for an object with initial velocity $v_0$ starting from $y=0 $ at $t=0$.\n",
    "\n",
    "$\\rightarrow $ Try to recover $g$ from noisy measurement on the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81\n",
    "v_0 = 3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $t$ and $y(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = torch.linspace(0, 1, 100, device=device)\n",
    "positions = -1./2. * g * times ** 2 + v_0 * times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some noisy measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = 0.2\n",
    "noise = amplitude * torch.from_numpy(np.random.sample(times.shape[0])).to(device) - amplitude / 2.\n",
    "noisy_positions = (positions + noise).float().to(device)\n",
    "\n",
    "num_measurements = 50\n",
    "idx = np.random.choice(np.arange(len(times)), num_measurements, replace=False)\n",
    "measurement_times = times[idx]\n",
    "measurements = noisy_positions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times.tolist(), positions.tolist(), label=\"True $y(t)$\")\n",
    "# plt.plot(times.tolist(), noisy_positions.tolist(), label=\"True with Noise\")\n",
    "plt.plot(measurement_times.tolist(), measurements.tolist(), \"x\", label= str(num_measurements)+ \" Measurements\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Height $y$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try to learn gravitation from the data\n",
    "# Constrct a Parameter such that PyTorch can optimize it:\n",
    "\n",
    "# Initialize with bad guess:\n",
    "g_param = torch.nn.Parameter(torch.tensor(g * 10, device=device))\n",
    "g_param = g_param.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn = Net(num_hidden_units=16, num_hidden_layers=2, inputs=1).to(device)\n",
    "\n",
    "# Don't forget to add g_param to the list of trainable parameters:\n",
    "params = list(y_nn.parameters()) + [g_param]\n",
    "\n",
    "loss_function=torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "lbfgs_optim = torch.optim.LBFGS(params,\n",
    "                                max_iter=max_iters,\n",
    "                                line_search_fn=\"strong_wolfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, define callables that return the losses for L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loss(nn, data_points, data_values):\n",
    "    \n",
    "    predictions = nn(data_points)\n",
    "    \n",
    "    return loss_function(predictions, data_values)\n",
    "\n",
    "def pde_loss(nn, residual_points, g):\n",
    "    \n",
    "    residual_points.requires_grad = True\n",
    "    \n",
    "    y = nn(residual_points)\n",
    "    \n",
    "    dy_dt, = torch.autograd.grad(outputs=y,\n",
    "                             inputs=residual_points,\n",
    "                             grad_outputs=torch.ones_like(residual_points),\n",
    "                             create_graph=True)\n",
    "\n",
    "    ddy_dtt, = torch.autograd.grad(outputs=dy_dt,\n",
    "                                 inputs=residual_points,\n",
    "                                 grad_outputs=torch.ones_like(residual_points),\n",
    "                                 create_graph=True) \n",
    "    # The ODE:\n",
    "    residual = ddy_dtt + g\n",
    "    \n",
    "    return loss_function(residual, torch.zeros_like(residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "g_values = []\n",
    "def closure():\n",
    "    \n",
    "    lbfgs_optim.zero_grad()\n",
    "    \n",
    "    data_loss_value = boundary_loss(y_nn, measurement_times, measurements)\n",
    "    \n",
    "    pde_loss_value = pde_loss(y_nn, times, g_param)\n",
    "    \n",
    "    loss = data_loss_value + pde_loss_value\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        \n",
    "    # Log both the loss and g during training:\n",
    "    losses.append(loss.item())\n",
    "    g_values.append(g_param.item())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.semilogy(losses)\n",
    "plt.show();\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"g during training, final g= \"+format(g_param.item(), \".2f\"), fontsize=14)\n",
    "plt.plot(g_values, label=\"g parameter\")\n",
    "plt.plot(np.zeros(len(g_values)) + g, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times.tolist(), positions.tolist(), marker=\"o\", markevery=14, label=\"True\");\n",
    "plt.plot(measurement_times.tolist(), measurements.tolist(), \"x\", label= str(num_measurements)+ \" Measurements\")\n",
    "plt.plot(times.tolist(), y_nn(times).tolist(), \"-\", marker=\"s\", markevery=10, label=\"NN\");\n",
    "plt.xlabel(\"time\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Height\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
