{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks and Automatic Differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "+ Automatic differentiation (in ML):\n",
    "Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research\n",
    "\n",
    "+ The M.Sc. thesis by Sebastian Mitusch (Simula) gives a nice intro to forward/backward automatic differentiation (and implementation for FEniCS in dolfin-adjoint):\n",
    "https://www.duo.uio.no/handle/10852/63505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to train a neural network using supervised learning\n",
    "\n",
    "\n",
    "## Hence we need:\n",
    "\n",
    "0. Input-output-pairs $(x_i,y_i)$\n",
    "\n",
    "1. Neural network $u^{NN}(\\theta) \\quad \\qquad \\qquad \\rightarrow \\qquad$ PyTorch: Tensors, Activation Functions, Parameters,  ... \n",
    "\n",
    "2. Loss function $L(\\theta) = \\sum (u(x_i)-y_i)^2$\n",
    "\n",
    "3. Algorithm to find $\\theta = \\text{argmin} L(\\theta) \\qquad \\rightarrow \\qquad $ Gradient-based optimization: SGD, L-BFGS\n",
    "\n",
    "4. Gradients: Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to seed the random number generators, so that we get the same random numbers every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "print(\"Setting random seed\")\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if PyTorch finds a GPU (has to be NVIDIA):\n",
    "\n",
    "Note that for small data sets and small networks, CPU will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU \", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data structure, almost like numpy.arrays, but with support fo GPU and automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100, device=device)\n",
    "\n",
    "# Can also create tensors from numpy arrays:\n",
    "# x = torch.from_numpy(np.linspace(0,1,100))\n",
    "# x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can often do things in numpy style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[x < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can transfer between python, numpy and torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2])\n",
    "x = x.numpy()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2])\n",
    "x = torch.from_numpy(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_hidden_units, num_hidden_layers, inputs, outputs=1):\n",
    "        \n",
    "        super(Net, self).__init__()        \n",
    "        \n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Dimensions of input/output\n",
    "        self.inputs =  inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        self.input_layer = torch.nn.Linear(self.inputs, self.num_hidden_units)\n",
    "        \n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(\n",
    "            self.num_hidden_units, self.num_hidden_units)\n",
    "            for i in range(self.num_hidden_layers - 1)])\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(self.num_hidden_units, self.outputs)\n",
    "        \n",
    "        # Use hyperbolic tangent as activation:\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "        self.activation_2 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"[Compute NN output]\n",
    "\n",
    "        Args:\n",
    "            x ([torch.Tensor]): input tensor\n",
    "        Returns:\n",
    "            [torch.Tensor]: [The NN output]\n",
    "        \"\"\"\n",
    "        # Transform the shape of the Tensor to match what is expected by torch.nn.Linear\n",
    "        x = torch.unsqueeze(x, 1) \n",
    "    \n",
    "        out = self.input_layer(x)\n",
    "        \n",
    "        # The first hidden layer:\n",
    "        out = self.activation_2(out)\n",
    "        \n",
    "        # print(out)\n",
    "\n",
    "        # The other hidden layers:\n",
    "        for i, linearLayer in enumerate(self.linear_layers):\n",
    "            out = linearLayer(out)\n",
    "            out = self.activation(out)\n",
    "\n",
    "        # No activation in the output layer:\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        out = torch.squeeze(out, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feedforward NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=1, inputs=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the trainable parameters (weights and biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the weights in one layer:\n",
    "u_nn.input_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of tunable parameter per Parameter object\n",
    "params = list(u_nn.parameters())\n",
    "print(list(map(lambda x: x.numel(), params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the output of the untrained NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn_x = u_nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(x.numpy(), u_nn_x.detach().numpy())\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"Network output\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Automatic (Algorithmic) differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Derivative of\n",
    "\n",
    "$ f(x) = \\exp(-3x^2) = v_4(v_3(v_2(v_1(x))))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Forward pass: Given $x$, compute       |      \n",
    "| ----| :----------------------------------------- | \n",
    "| $\\downarrow$ | $v_1 = x^2$   | \n",
    "| $\\downarrow$| $v_2 = 3 v_1$ | \n",
    "| $\\downarrow$| $v_3 = - v_2$ \n",
    "| $\\downarrow$| $v_4 = \\exp v_3$ | \n",
    "|  $\\downarrow$| $f = v_4 $ |$ \\overline{v_4} = \\frac{df }{d v_4} = 1$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the chain rule:\n",
    "\n",
    "$\\frac{df}{dx} =\\frac{d f}{d v_4} \\frac{d v_4}{d v_3}\\frac{d v_3}{d v_2}\\frac{d v_2}{d v_1}\\frac{d v_1}{dx}$\n",
    "\n",
    "and the fact that all the derivatives $\\frac{d v_{i+1}}{d v_i}$ of the **elementary operations $v_{i+1}(v_i)$** can be considered known!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to have $\\frac{df}{dx}$;\n",
    "\n",
    "Let us define the adjoint variable $ \\overline{v_i} = \\frac{df}{dv_i}$ and start from  $ \\overline{v_4}$ to compute the other $ \\overline{v_i}$, utilizing \n",
    "+ the stored values $v_i$\n",
    "+ Known (part of the code base) derivatives $\\frac{d v_{i+1}}{d v_i}$ of the elementary operations $v_{i+1}(v_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"foo\">\n",
    "    \n",
    "|  <div style=\"width:200px\">Forward pass</div>        |           <div style=\"width:350px\">Backward pass</div>               | |\n",
    "| :----------------------------------------- | :----------------------------------------------------------| -----|\n",
    "| $v_1 = x^2$   | $ \\overline{x} = \\frac{d f}{d x} = \\overline{v_1}  \\frac{d v_1}{d x}= -3 \\exp (-3 x^2) \\cdot 2 x $ |$\\uparrow $ |\n",
    "| $v_2 = 3 v_1$ | $ \\overline{v_1} = \\overline{v_2}  \\frac{d v_2}{d v_1}= -\\exp (v_3) \\cdot 3$ |  $\\uparrow $ |\n",
    "| $v_3 = - v_2$ |$ \\overline{v_2} = \\overline{v_3}  \\frac{d v_3}{d v_2}= \\exp (v_3) \\cdot (-1)$ |$\\uparrow $ |\n",
    "| $v_4 = \\exp v_3$ | $ \\overline{v_3}= \\frac{d f}{d v_3} = \\frac{d f}{d v_4} \\frac{d v_4}{d v_3} =  \\overline{v_4}  \\frac{d v_4}{d v_3}= 1 \\cdot \\exp \\color{red} v_{ \\color{red} 3}  $ |$\\uparrow $ |\n",
    "| $f = v_4 $ |$ \\overline{f} = \\overline{v_4} = \\frac{df }{d v_4} = 1$| $\\uparrow $ |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be used to compute derivatives with respect to parameters:\n",
    "\n",
    "$ v_2 = \\theta \\cdot v_1 \\rightarrow  f_{\\theta}(x) = \\exp(-\\theta x^2)$\n",
    "\n",
    "$\\rightarrow \\frac{df_{\\theta}}{d \\theta} = \\frac{d f}{d v_4} \\frac{d v_4}{d v_3} \\frac{d v_3}{d v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} v_1 \n",
    "= - \\exp (- \\theta x^2 ) \\cdot  x^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor(-42., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, theta):\n",
    "    return torch.exp(- theta * x **2)\n",
    "\n",
    "def df_dtheta(x, theta):\n",
    "    return - torch.exp(- theta * x **2) * x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(-1.)\n",
    "\n",
    "x.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the grad ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, nothing has been done with $x$ so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is the grad $\\frac{\\partial f}{\\partial \\theta}$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this correct ?\n",
    "\n",
    "$\\frac{df_{\\theta}}{d \\theta} = - \\exp (- \\theta x^2 ) \\cdot  x^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtheta(x, theta) == theta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch autograd to compute derivatives of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 100, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes: \n",
    "\n",
    "+ `requires_grad=True` tells PyTorch to keep track of intermediate results (\"create a computational graph\")\n",
    "\n",
    "+ In-place operations `x *= x` are not supported by autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2 # torch.sin(np.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx, = torch.autograd.grad(outputs=y,\n",
    "                             inputs=x,\n",
    "                             grad_outputs=torch.ones_like(y),\n",
    "                             create_graph=True)\n",
    "\n",
    "ddy_dxx, = torch.autograd.grad(outputs=dy_dx,\n",
    "                             inputs=x,\n",
    "                             grad_outputs=torch.ones_like(dy_dx),\n",
    "                             create_graph=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddy_dxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(bar):\n",
    "    return (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , u = foo(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark on the last arguments:\n",
    "\n",
    "1. `grad_outputs=torch.ones_like(y)` is required for non-scalar outputs, technical reasons related to backward automatic differentiation\n",
    "2. We need to set `create_graph=True` if we want to compute higher order derivatives such that PyTorch adds its computation of `dy_dx` to the computational graph (The same applies if we want to include derivatives into the loss function, because then we need derivatives with respect to NN weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sometimes want to convert the torch.Tensor back to python types (or numpy arrays):\n",
    "# Note that a GPU tensor needs to be moved to CPU with the .cpu() call\n",
    "\n",
    "dy_dx_list = dy_dx.tolist()\n",
    "dy_dx_np = dy_dx.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(x.tolist(), y.tolist(), label=\"y(x)\") \n",
    "plt.plot(x.tolist(), dy_dx.tolist(), label=\"dy/dx\") \n",
    "plt.plot(x.tolist(), ddy_dxx.tolist(), label=\"ddy/dxx\") \n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based optimization in a nutshell:\n",
    "\n",
    "while criterion is True:\n",
    "   1. Compute loss $L(\\theta, \\text{inputs})$\n",
    "   2. Update the parameters as $\\theta \\hookleftarrow \\theta - \\eta \\nabla_{\\theta} L (\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's \"train\" a NN to to fit the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(0, 1, 100, device=device)\n",
    "\n",
    "targets = torch.sin(np.pi * inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=1, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=params,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=torch.nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iters):\n",
    "    #  Free all intermediate values:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass:\n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    # Compute the MSE loss:\n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    # Backward pass, compute gradient w.r.t. weights and biases\n",
    "    loss.backward()\n",
    "    \n",
    "    # Log the loss to make a figure\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.semilogy(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), label=\"Network\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), label=\"Target\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This did not work too well...\n",
    "# What could have gone wrong ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can look at the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn.input_layer.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be used to compute derivatives with respect to parameters:\n",
    "\n",
    "$ v_2 = \\theta \\cdot v_1 \\rightarrow  f_{\\theta}(x) = \\exp(-\\theta x^2)$\n",
    "\n",
    "$\\rightarrow \\frac{df_{\\theta}}{d \\theta} = \\frac{d f}{d v_4} \\frac{d v_4}{d v_3} \\frac{d v_3}{d v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} \\frac{d v_2}{d \\theta} = \\overline{v_2} v_1 \n",
    "= - \\exp (- \\theta x^2 ) \\cdot  x^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a second-order method like L-BFGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=1, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim = torch.optim.LBFGS(params,\n",
    "                                max_iter=max_iters,\n",
    "                                line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L-BFGS needs to be used slightly different than first order methods in PyTorch:\n",
    "\n",
    "The iteration happens inside `lbfgs_optim.step()`, so it is sufficient to call `lbfgs_optim.step()` only once, and specify the number of iterations with the argument `max_iter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS requires one to define a method (called \"closure\" in the docs) \n",
    "# that evaluates the model and returns the loss:\n",
    "\n",
    "def closure(loss_function=torch.nn.MSELoss(reduction=\"mean\")):\n",
    "    \n",
    "    lbfgs_optim.zero_grad()\n",
    "    \n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        \n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call L-BFGS and plot the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.semilogy(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), marker=\"o\", markevery=5, label=\"Network $y^{NN}(x)$\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), marker=\"s\", markevery=10, label=\"Target $y(x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does the network perform outside the training domain $[0,1]$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-1, 2, 100)\n",
    "y_test = torch.sin(torch.pi * x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "ax = plt.gca()\n",
    "plt.plot(x_test.tolist(), u_nn(x_test).tolist(), marker=\"o\", markevery=5, label=\"Network $y^{NN}(x)$\")\n",
    "plt.plot(x_test.tolist(), y_test.tolist(), marker=\"s\", markevery=10, label=\"Target $y(x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.axvspan(0, 1, color='k', alpha=0.2)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try something more challenging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 10\n",
    "inputs = torch.linspace(0, 1, 100)\n",
    "targets = torch.sin(torch.pi * omega * inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=8, num_hidden_layers=1, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "optimizer = torch.optim.Adam(params=params)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    #  Free all intermediate values:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass:\n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    # Compute the MSE loss:\n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    # Backward pass, compute gradient w.r.t. weights and biases\n",
    "    loss.backward()\n",
    "    \n",
    "    # Log the loss to make a figure\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    optimizer.step()\n",
    "    \n",
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.semilogy(losses);\n",
    "plt.xlabel(\"Iter\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), marker=\"o\", markevery=5, label=\"Network $y^{NN}(x)$\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), marker=\"s\", markevery=10, label=\"Target $y(x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we make it work with a better optimizer ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_nn = Net(num_hidden_units=16, num_hidden_layers=3, inputs=1).to(device)\n",
    "params = list(u_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for x in list(u_nn.parameters()):\n",
    "    print(x.shape)\n",
    "    n += x.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS requires one to define a method (called \"closure\" in the docs) \n",
    "# that evaluates the model and returns the loss:\n",
    "\n",
    "def closure(loss_function=torch.nn.MSELoss(reduction=\"mean\")):\n",
    "    \n",
    "    lbfgs_optim.zero_grad()\n",
    "    \n",
    "    predictions = u_nn(inputs)\n",
    "    \n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    if loss.requires_grad:\n",
    "        loss.backward()\n",
    "        \n",
    "    losses.append(loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim = torch.optim.LBFGS(params,\n",
    "                                max_iter=max_iters,\n",
    "                                line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "lbfgs_optim.step(closure)\n",
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.semilogy(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4),dpi=200)\n",
    "plt.plot(inputs.tolist(), u_nn(inputs).tolist(), marker=\"o\", markevery=5, label=\"Network $y^{NN}(x)$\")\n",
    "plt.plot(inputs.tolist(), targets.tolist(), marker=\"s\", markevery=10, label=\"Target $y(x)$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
